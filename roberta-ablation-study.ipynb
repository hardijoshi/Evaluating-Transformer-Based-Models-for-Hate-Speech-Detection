{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom datasets import Dataset\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:11.581413Z","iopub.execute_input":"2025-02-21T00:47:11.581789Z","iopub.status.idle":"2025-02-21T00:47:20.405890Z","shell.execute_reply.started":"2025-02-21T00:47:11.581754Z","shell.execute_reply":"2025-02-21T00:47:20.404982Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:20.407103Z","iopub.execute_input":"2025-02-21T00:47:20.407601Z","iopub.status.idle":"2025-02-21T00:47:20.482122Z","shell.execute_reply.started":"2025-02-21T00:47:20.407569Z","shell.execute_reply":"2025-02-21T00:47:20.481108Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the tokenizer and the RoBERTa model with 2 labels for binary classification\nmodel_name = 'roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:20.484302Z","iopub.execute_input":"2025-02-21T00:47:20.484602Z","iopub.status.idle":"2025-02-21T00:47:41.505456Z","shell.execute_reply.started":"2025-02-21T00:47:20.484575Z","shell.execute_reply":"2025-02-21T00:47:41.504565Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df7d6116a024574a91da0320df5416e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a8819cb374d493bac23675c6c4b9acf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f1fd022293f423784bf1b81ac7f16d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83b0e1b218f64e958bb41aba30a9cb50"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def load_data():\n    train_data = pd.read_csv(\"/kaggle/input/hate-comments/balanced_comments_dataset.csv\")\n    test_data = pd.read_csv(\"/kaggle/input/hate-comments/balanced_test_dataset.csv\")\n    train_data['comment'] = train_data['comment'].astype(str)\n    test_data['comment'] = test_data['comment'].astype(str)\n    return train_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:41.506856Z","iopub.execute_input":"2025-02-21T00:47:41.507223Z","iopub.status.idle":"2025-02-21T00:47:41.511521Z","shell.execute_reply.started":"2025-02-21T00:47:41.507192Z","shell.execute_reply":"2025-02-21T00:47:41.510654Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples['comment'], padding='max_length', truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:41.512270Z","iopub.execute_input":"2025-02-21T00:47:41.512589Z","iopub.status.idle":"2025-02-21T00:47:41.524855Z","shell.execute_reply.started":"2025-02-21T00:47:41.512567Z","shell.execute_reply":"2025-02-21T00:47:41.524037Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def prepare_data(train_data, test_data, use_weighted_sampler=True):\n    train_dataset = Dataset.from_pandas(train_data[['comment', 'toxic']])\n    test_dataset = Dataset.from_pandas(test_data[['comment', 'toxic']])\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    test_dataset = test_dataset.map(tokenize_function, batched=True)\n    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'toxic'])\n    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'toxic'])\n   \n    batch_size = 8\n    if use_weighted_sampler:\n        class_counts = train_data['toxic'].value_counts()\n        sample_weights = [1.0 / class_counts[c] for c in train_data['toxic']]\n        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n    else:\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n   \n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    return train_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:41.525737Z","iopub.execute_input":"2025-02-21T00:47:41.526045Z","iopub.status.idle":"2025-02-21T00:47:41.536598Z","shell.execute_reply.started":"2025-02-21T00:47:41.526012Z","shell.execute_reply":"2025-02-21T00:47:41.535877Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(config_name, use_mixed_precision=True, use_grad_accum=True, use_early_stopping=True, use_weighted_sampler=True):\n    train_data, test_data = load_data()\n    train_loader, test_loader = prepare_data(train_data, test_data, use_weighted_sampler)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n   \n    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    num_epochs = 5\n    num_training_steps = num_epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n   \n    loss_fn = nn.CrossEntropyLoss()\n    scaler = torch.cuda.amp.GradScaler() if use_mixed_precision else None\n    accumulation_steps = 4 if use_grad_accum else 1\n    early_stopping_patience = 3 if use_early_stopping else None\n   \n    best_val_loss = float('inf')\n    patience_counter = 0\n   \n    start_time = time.time()\n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n        for i, batch in enumerate(train_loader):\n            optimizer.zero_grad()\n            batch = {k: v.to(device) for k, v in batch.items()}\n           \n            if use_mixed_precision:\n                with torch.cuda.amp.autocast():\n                    outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['toxic'])\n                    loss = outputs.loss / accumulation_steps\n                scaler.scale(loss).backward()\n            else:\n                outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['toxic'])\n                loss = outputs.loss / accumulation_steps\n                loss.backward()\n           \n            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n                if use_mixed_precision:\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    optimizer.step()\n                lr_scheduler.step()\n           \n            total_train_loss += loss.item() * accumulation_steps\n       \n        avg_train_loss = total_train_loss / len(train_loader)\n        model.eval()\n        total_val_loss = 0\n        all_preds, all_labels = [], []\n       \n        with torch.no_grad():\n            for batch in test_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['toxic'])\n                total_val_loss += outputs.loss.item()\n                preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n                labels = batch['toxic'].cpu().numpy()\n                all_preds.extend(preds)\n                all_labels.extend(labels)\n       \n        avg_val_loss = total_val_loss / len(test_loader)\n        accuracy = accuracy_score(all_labels, all_preds)\n        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n       \n        print(f\"{config_name} | Epoch {epoch + 1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}, Acc {accuracy:.4f}, F1 {f1:.4f}\")\n       \n        if use_early_stopping and avg_val_loss >= best_val_loss:\n            patience_counter += 1\n            if patience_counter >= early_stopping_patience:\n                print(f\"{config_name}: Early stopping triggered\")\n                break\n        else:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n   \n    training_time = time.time() - start_time\n    return accuracy, f1, training_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:41.537513Z","iopub.execute_input":"2025-02-21T00:47:41.537813Z","iopub.status.idle":"2025-02-21T00:47:41.552275Z","shell.execute_reply.started":"2025-02-21T00:47:41.537782Z","shell.execute_reply":"2025-02-21T00:47:41.551590Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def run_ablation_study():\n    configs = [\n        (\"Full Model\", True, True, True, True),\n        (\"Without Mixed Precision\", False, True, True, True),\n        (\"Without Gradient Accumulation\", True, False, True, True),\n        (\"Without Early Stopping\", True, True, False, True),\n        (\"Basic Fine-tuning\", False, False, False, False)\n    ]\n   \n    results = []\n    for config_name, mixed_precision, grad_accum, early_stopping, weighted_sampler in configs:\n        acc, f1, train_time = train_model(config_name, mixed_precision, grad_accum, early_stopping, weighted_sampler)\n        results.append((config_name, acc, f1, train_time))\n   \n    print(\"\\nAblation Study Results\")\n    print(\"Configuration | Accuracy | F1 Score | Training Time\")\n    for res in results:\n        print(f\"{res[0]} | {res[1]*100:.2f}% | {res[2]:.2f} | {res[3]:.2f}x\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:47:41.552965Z","iopub.execute_input":"2025-02-21T00:47:41.553201Z","iopub.status.idle":"2025-02-21T00:47:41.567917Z","shell.execute_reply.started":"2025-02-21T00:47:41.553182Z","shell.execute_reply":"2025-02-21T00:47:41.567129Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"run_ablation_study()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T03:42:26.718244Z","iopub.execute_input":"2025-02-21T03:42:26.718540Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d1e485a04c1435ab7c178697eb72395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43229b7e0df946b887a031faaf04d689"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-8-5101595eafcc>:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if use_mixed_precision else None\n<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Full Model | Epoch 1: Train Loss 0.2461, Val Loss 0.2756, Acc 0.8857, F1 0.8958\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Full Model | Epoch 2: Train Loss 0.1643, Val Loss 0.4241, Acc 0.8977, F1 0.9057\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Full Model | Epoch 3: Train Loss 0.1572, Val Loss 0.2592, Acc 0.9080, F1 0.9134\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Full Model | Epoch 4: Train Loss 0.1364, Val Loss 0.3037, Acc 0.8972, F1 0.9052\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Full Model | Epoch 5: Train Loss 0.1200, Val Loss 0.3586, Acc 0.8979, F1 0.9053\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb703b597b284d6ab6b637b775c61c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae2a0edc86d4a61bb5bc399c0088aeb"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Without Mixed Precision | Epoch 1: Train Loss 0.2512, Val Loss 0.2466, Acc 0.9072, F1 0.9112\nWithout Mixed Precision | Epoch 2: Train Loss 0.1788, Val Loss 0.3631, Acc 0.8782, F1 0.8905\nWithout Mixed Precision | Epoch 3: Train Loss 0.1532, Val Loss 0.3387, Acc 0.8839, F1 0.8947\nWithout Mixed Precision | Epoch 4: Train Loss 0.1376, Val Loss 0.4129, Acc 0.8818, F1 0.8932\nWithout Mixed Precision: Early stopping triggered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86e794d91a7b4e4883a3de7ae2e49e83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4362d74a9eb3450a92d4a31f7f2b7911"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-8-5101595eafcc>:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if use_mixed_precision else None\n<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Gradient Accumulation | Epoch 1: Train Loss 0.1834, Val Loss 0.2945, Acc 0.9031, F1 0.9095\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Gradient Accumulation | Epoch 2: Train Loss 0.1062, Val Loss 0.4376, Acc 0.8868, F1 0.8975\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Gradient Accumulation | Epoch 3: Train Loss 0.0741, Val Loss 0.4743, Acc 0.9040, F1 0.9107\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Gradient Accumulation | Epoch 4: Train Loss 0.0457, Val Loss 0.4858, Acc 0.9021, F1 0.9091\nWithout Gradient Accumulation: Early stopping triggered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1994c0699a79458f9ab95de5b80dd1e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46077f82bf4e43879b5a7abd4ebadc66"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-8-5101595eafcc>:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if use_mixed_precision else None\n<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Early Stopping | Epoch 1: Train Loss 0.2611, Val Loss 0.2846, Acc 0.8966, F1 0.9038\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Early Stopping | Epoch 2: Train Loss 0.1842, Val Loss 0.3129, Acc 0.8961, F1 0.9040\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Early Stopping | Epoch 3: Train Loss 0.1555, Val Loss 0.2697, Acc 0.8975, F1 0.9028\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Early Stopping | Epoch 4: Train Loss 0.1351, Val Loss 0.3100, Acc 0.9008, F1 0.9073\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-5101595eafcc>:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Without Early Stopping | Epoch 5: Train Loss 0.1230, Val Loss 0.3787, Acc 0.8875, F1 0.8976\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"262b0f8efda547b58ee2f8f30e2e55b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e3b2dff8064e6ab38af99300e3faa3"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Basic Fine-tuning | Epoch 1: Train Loss 0.1953, Val Loss 0.2431, Acc 0.9080, F1 0.9140\nBasic Fine-tuning | Epoch 2: Train Loss 0.1163, Val Loss 0.3129, Acc 0.9053, F1 0.9119\nBasic Fine-tuning | Epoch 3: Train Loss 0.0710, Val Loss 0.3949, Acc 0.8918, F1 0.9010\nBasic Fine-tuning | Epoch 4: Train Loss 0.0373, Val Loss 0.4874, Acc 0.8980, F1 0.9055\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}